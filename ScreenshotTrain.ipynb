{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ScreenshotTrain.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYw6Lek8IKdR",
        "colab_type": "code",
        "outputId": "d8791044-2eb5-4fca-fc3e-6bd803591ce1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from os import listdir\n",
        "from numpy import array\n",
        "from keras.preprocessing.text import Tokenizer, one_hot\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Model, Sequential, model_from_json\n",
        "from keras.utils import to_categorical\n",
        "from keras.layers.core import Dense, Dropout, Flatten\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers.convolutional import Conv2D\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras.layers import Embedding, TimeDistributed, RepeatVector, LSTM, concatenate , Input, Reshape, Dense,GRU\n",
        "from keras.preprocessing.image import array_to_img, img_to_array, load_img\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oOUV4qBJIQQ-",
        "colab_type": "code",
        "outputId": "dc12f915-1d63-4dfe-c7bf-20395b79abf4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6gxzoLwISdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#load dataset and organize for easy parsing\n",
        "#DONE: train1\n",
        "#helper function: read file and return txt as a string\n",
        "def load_doc(filename):\n",
        "    with open(filename,'r') as file:\n",
        "      text = file.read()\n",
        "      file.close()\n",
        "    return text\n",
        "\n",
        "def load_data():\n",
        "    text = []\n",
        "    images = []\n",
        "    #load all files + sort them\n",
        "    all_filenames = listdir(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train16\")\n",
        "    all_filenames.sort()\n",
        "    for filename in (all_filenames):\n",
        "        if filename[-3:]==\"npz\":\n",
        "            #load imgs already prepped in arr\n",
        "            image = np.load(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train16/\"+filename)\n",
        "            images.append(image['features'])\n",
        "        else:\n",
        "            #load bootstrap tokens LABELS/TARGETS\n",
        "            syntax = '<START>' + load_doc(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train16/\"+filename) + '<END'\n",
        "            syntax = ' '.join(syntax.split())\n",
        "            syntax = syntax.replace(',',' ,')\n",
        "            text.append(syntax)\n",
        "            \n",
        "    images = np.array(images,dtype=float)\n",
        "   # image_File = np.save('/content/drive/My Drive/Colab Notebooks/ScreenshotTest/image_file',images)\n",
        "   # text_file = np.save('/content/drive/My Drive/Colab Notebooks/ScreenshotTest/text_fie',text)\n",
        "    return images,text\n",
        "  \n",
        "train_features, texts = load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nBsEBoiaIfxe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = Tokenizer(filters='',split=\" \",lower=False)\n",
        "tokenizer.fit_on_texts([load_doc('/content/drive/My Drive/Colab Notebooks/ScreenshotTest/resources/bootstrap.vocab')])\n",
        "\n",
        "vocab_size = len(tokenizer.word_index)+1\n",
        "#map input sentences into vocabulary indexes\n",
        "train_sequences = tokenizer.texts_to_sequences(texts)\n",
        "#longest set of bootstrap tok\n",
        "max_sequence = max(len(s) for s in train_sequences)\n",
        "max_length = 48\n",
        "\n",
        "\n",
        "def preprocess_data(sequences, features):\n",
        "    X, y, image_data = list(), list(), list()\n",
        "    for img_no, seq in enumerate(sequences):\n",
        "        for i in range(1, len(seq)):\n",
        "            # Add the sentence until the current count(i) and add the current count to the output\n",
        "            in_seq, out_seq = seq[:i], seq[i]\n",
        "            # Pad all the input token sentences to max_sequence\n",
        "            in_seq = pad_sequences([in_seq], maxlen=max_sequence)[0]\n",
        "            # Turn the output into one-hot encoding\n",
        "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
        "            # Add the corresponding image to the boostrap token file\n",
        "            image_data.append(features[img_no])\n",
        "            # Cap the input sentence to 48 tokens and add it\n",
        "            X.append(in_seq[-48:])\n",
        "            y.append(out_seq)\n",
        "    return np.array(X), np.array(y), np.array(image_data)\n",
        "\n",
        "X, y, image_data = preprocess_data(train_sequences, train_features)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVxKFxKwK6Q-",
        "colab_type": "code",
        "outputId": "a65c3e9b-7fbe-44bb-c211-78b171fab72c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "#Create the encoder\n",
        "#image_model = Sequential()\n",
        "#image_model.add(Conv2D(16, (3, 3), padding='valid', activation='relu', input_shape=(256, 256, 3,)))\n",
        "#image_model.add(Conv2D(16, (3,3), activation='relu', padding='same', strides=2))\n",
        "#image_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
        "#image_model.add(Conv2D(32, (3,3), activation='relu', padding='same', strides=2))\n",
        "#image_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
        "#image_model.add(Conv2D(64, (3,3), activation='relu', padding='same', strides=2))\n",
        "#image_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))\n",
        "#image_model.add(MaxPooling2D(pool_size=(2, 2), strides=(2,2)))\n",
        "\n",
        "#image_model.add(Flatten())\n",
        "#image_model.add(Dense(1024, activation='relu'))\n",
        "#image_model.add(Dropout(0.3))\n",
        "#image_model.add(Dense(512, activation='relu'))\n",
        "#image_model.add(Dropout(0.3))\n",
        "\n",
        "#image_model.add(RepeatVector(max_length))\n",
        "\n",
        "#visual_input = Input(shape=(256, 256, 3,))\n",
        "#encoded_image = image_model(visual_input)\n",
        "\n",
        "#language_input = Input(shape=(max_length,))\n",
        "#language_model = Embedding(vocab_size, 50, input_length=max_length, mask_zero=True)(language_input)\n",
        "#language_model = GRU(128, return_sequences=True)(language_model)\n",
        "#language_model = GRU(128, return_sequences=True)(language_model)\n",
        "\n",
        "#Create the decoder\n",
        "#decoder = concatenate([encoded_image, language_model])\n",
        "#decoder = GRU(512, return_sequences=True)(decoder)\n",
        "#decoder = GRU(512, return_sequences=False)(decoder)\n",
        "#decoder = Dense(vocab_size, activation='softmax')(decoder)\n",
        "\n",
        "#LOAD MODEL FROM JSON\n",
        "with open('/content/drive/My Drive/Colab Notebooks/ScreenshotTest/modelGRU.json','r') as json_file:\n",
        "  model_json = json_file.read()\n",
        "  json_file.close()\n",
        "model = model_from_json(model_json)\n",
        "model.load_weights(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/weightsGRU.h5\")\n",
        "print(\"Loaded model from disk\")\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3239: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Loaded model from disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nutgklHqIkYg",
        "colab_type": "code",
        "outputId": "d0742055-aea0-4372-9e0a-7502299a9f92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "# Compile the model\n",
        "#model = Model(inputs=[visual_input, language_input], outputs=decoder)\n",
        "\n",
        "optimizer = RMSprop(lr=0.0001, clipvalue=1.0)\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "#model.summary()\n",
        "                "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Os3mjciy2r3e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#save data\n",
        "import pickle as pickle\n",
        "epoch_file = '/content/drive/My Drive/Colab Notebooks/ScreenshotTest/epochsDataMyModel28.txt'\n",
        "data_file='/content/drive/My Drive/Colab Notebooks/ScreenshotTest/data1CategoricalMyModel28.txt'\n",
        "\n",
        "report_data = {\n",
        "    \"accuracy\":[],\n",
        "    \"val_accuracy\":[],\n",
        "    \"loss\":[],\n",
        "    \"val_loss\":[]\n",
        "}\n",
        "num_epoch =0\n",
        "\n",
        "def updateEpoch(epoch, logs):\n",
        "  to_save = num_epoch + epoch + 1\n",
        "  report_data['accuracy'].append(logs['categorical_accuracy'])\n",
        "  report_data['loss'].append(logs['loss'])\n",
        "  report_data['val_accuracy'].append(logs['val_categorical_accuracy'])\n",
        "  report_data['val_loss'].append(logs['val_loss'])\n",
        "  with open(epoch_file, \"w\") as file:  \n",
        "    file.write(str(to_save))\n",
        "  with open(data_file, \"wb\") as file:\n",
        "    pickle.dump(report_data, file)\n",
        "  #print(epoch, logs)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAQrQByiJMGY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import LambdaCallback\n",
        "#Save the model for every 2nd epoch\n",
        "filepath=\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\"\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_weights_only=True, period=2)\n",
        "lambdaCall = LambdaCallback(on_epoch_end=updateEpoch)\n",
        "callbacks_list = [checkpoint,lambdaCall]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzXA34iQqhlE",
        "colab_type": "code",
        "outputId": "4271a999-d5e0-4d16-f32b-e5eb5224537b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Train the model\n",
        "model.fit([image_data, X], y, batch_size=32, shuffle=False, validation_split=0.1, callbacks=callbacks_list, verbose=1, epochs=40)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Train on 3107 samples, validate on 346 samples\n",
            "Epoch 1/40\n",
            "3107/3107 [==============================] - 70s 22ms/step - loss: 0.3920 - categorical_accuracy: 0.9292 - val_loss: 0.3703 - val_categorical_accuracy: 0.9364\n",
            "Epoch 2/40\n",
            "3107/3107 [==============================] - 64s 20ms/step - loss: 0.1956 - categorical_accuracy: 0.9453 - val_loss: 0.2561 - val_categorical_accuracy: 0.9422\n",
            "\n",
            "Epoch 00002: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 3/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.1287 - categorical_accuracy: 0.9611 - val_loss: 0.2602 - val_categorical_accuracy: 0.9422\n",
            "Epoch 4/40\n",
            "3107/3107 [==============================] - 63s 20ms/step - loss: 0.0930 - categorical_accuracy: 0.9691 - val_loss: 0.2641 - val_categorical_accuracy: 0.9422\n",
            "\n",
            "Epoch 00004: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 5/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0709 - categorical_accuracy: 0.9723 - val_loss: 0.2778 - val_categorical_accuracy: 0.9422\n",
            "Epoch 6/40\n",
            "3107/3107 [==============================] - 63s 20ms/step - loss: 0.0533 - categorical_accuracy: 0.9797 - val_loss: 0.2929 - val_categorical_accuracy: 0.9364\n",
            "\n",
            "Epoch 00006: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 7/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0448 - categorical_accuracy: 0.9823 - val_loss: 0.2936 - val_categorical_accuracy: 0.9306\n",
            "Epoch 8/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0354 - categorical_accuracy: 0.9878 - val_loss: 0.2947 - val_categorical_accuracy: 0.9393\n",
            "\n",
            "Epoch 00008: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 9/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0360 - categorical_accuracy: 0.9897 - val_loss: 0.3028 - val_categorical_accuracy: 0.9306\n",
            "Epoch 10/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0305 - categorical_accuracy: 0.9897 - val_loss: 0.2742 - val_categorical_accuracy: 0.9335\n",
            "\n",
            "Epoch 00010: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 11/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0226 - categorical_accuracy: 0.9923 - val_loss: 0.2992 - val_categorical_accuracy: 0.9335\n",
            "Epoch 12/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0197 - categorical_accuracy: 0.9952 - val_loss: 0.3233 - val_categorical_accuracy: 0.9335\n",
            "\n",
            "Epoch 00012: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 13/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0191 - categorical_accuracy: 0.9936 - val_loss: 0.3488 - val_categorical_accuracy: 0.9306\n",
            "Epoch 14/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0168 - categorical_accuracy: 0.9961 - val_loss: 0.3242 - val_categorical_accuracy: 0.9335\n",
            "\n",
            "Epoch 00014: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 15/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0168 - categorical_accuracy: 0.9965 - val_loss: 0.3134 - val_categorical_accuracy: 0.9335\n",
            "Epoch 16/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0059 - categorical_accuracy: 0.9990 - val_loss: 0.3428 - val_categorical_accuracy: 0.9277\n",
            "\n",
            "Epoch 00016: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 17/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0111 - categorical_accuracy: 0.9965 - val_loss: 0.3821 - val_categorical_accuracy: 0.9306\n",
            "Epoch 18/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0087 - categorical_accuracy: 0.9971 - val_loss: 0.3350 - val_categorical_accuracy: 0.9249\n",
            "\n",
            "Epoch 00018: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 19/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0110 - categorical_accuracy: 0.9961 - val_loss: 0.3380 - val_categorical_accuracy: 0.9191\n",
            "Epoch 20/40\n",
            "3107/3107 [==============================] - 64s 21ms/step - loss: 0.0091 - categorical_accuracy: 0.9974 - val_loss: 0.4248 - val_categorical_accuracy: 0.9249\n",
            "\n",
            "Epoch 00020: saving model to /content/drive/My Drive/Colab Notebooks/ScreenshotTest/train/train1/weightstrainEDITGRU.best.hdf5\n",
            "Epoch 21/40\n",
            "3107/3107 [==============================] - 65s 21ms/step - loss: 0.0080 - categorical_accuracy: 0.9981 - val_loss: 0.3759 - val_categorical_accuracy: 0.9249\n",
            "Epoch 22/40\n",
            "2880/3107 [==========================>...] - ETA: 4s - loss: 0.0059 - categorical_accuracy: 0.9979"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4oVlEH1qmdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_json = model.to_json()\n",
        "with open(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/modelGRU.json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "# serialize weights to HDF5\n",
        "model.save_weights(\"/content/drive/My Drive/Colab Notebooks/ScreenshotTest/weightsGRU.h5\")\n",
        "print(\"Saved model to disk\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0cEocv5rC1p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Done! 6:39PM\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaJhYf2X-mPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"Done! \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ssp0K91FaOTb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KM2Yr8ltH6ES",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#summarize for accuracy\n",
        "plt.plot(report_data['accuracy'])\n",
        "plt.plot(report_data['val_accuracy'])\n",
        "plt.title('Accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('iterations')\n",
        "plt.legend(['training','target'],loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "#summarize for errors\n",
        "plt.plot(report_data['loss'])\n",
        "plt.plot(report_data['val_loss'])\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('iterations')\n",
        "plt.title('Learning Rate')\n",
        "plt.legend(['training','target'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ivqf3-OCHxY9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUg6ISROILR1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}